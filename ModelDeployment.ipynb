{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "- ML techniques are not limited to offline application and analyses\n",
    "- they have become predictive engine of various web services\n",
    "    - spam detection, search engines, recommendation systems, etc.\n",
    "    - online demo CNN for digit recognition: https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html \n",
    "- the goal of this chapter is to learn how to deploy a trained model and use it to classify new samples and also continuously learn from data in real time\n",
    "\n",
    "## Working with bigger data\n",
    "- it's normal to have hundreds of thousands of samples in dataset e.g. in text classification problems\n",
    "- in the era of big data (terabyes and petabytes), it's not uncommon to have dataset that doesn't fit in the desktop computer memory\n",
    "- either employ supercomputers or apply **out-of-core learning** with online algorithms\n",
    "- see https://scikit-learn.org/0.15/modules/scaling_strategies.html\n",
    "\n",
    "### out-of-core learning\n",
    "- allows us to work with large datasets by fitting the classifier incrementally on smaller batches of a dataset\n",
    "    \n",
    "### online algorithms\n",
    "- algorithms that don't need all the training samples at once but can be trained in batches over time\n",
    "    - also called incremental algorithms\n",
    "- these algorithms have `partial_fit` method in sci-kit learn framework\n",
    "- use **stochastic gradient descent** optimization algorithm that updates the models's weights using one example at a time\n",
    "- let's use `partial_fit` method of incremental SGDClassifier to train a logistric regression model using small mini-batches of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/movie_data.csv exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "# check if file exists otherwise download and unzip the zipped imdb dataset\n",
    "file = os.path.join('data', 'movie_data.csv')\n",
    "if not os.path.isfile(file):\n",
    "    if not os.path.isfile('movie_data.csv.gz'):\n",
    "        print('Please place a copy of the movie_data.csv.gz'\n",
    "              'in this directory. You can obtain it by'\n",
    "              'a) executing the code in the beginning of this'\n",
    "              'notebook or b) by downloading it from GitHub:'\n",
    "              'https://github.com/rasbt/python-machine-learning-'\n",
    "              'book-2nd-edition/blob/master/code/ch08/movie_data.csv.gz')\n",
    "    else:\n",
    "        with gzip.open('movie_data.csv.gz', 'rb') as in_f, \\\n",
    "                open(file, 'wb') as out_f:\n",
    "            out_f.write(in_f.read())\n",
    "else:\n",
    "    print(f'File {file} exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# The `stop` is defined as earlier in this chapter\n",
    "# Added it here for convenience, so that this section\n",
    "# can be run as standalone without executing prior code\n",
    "# in the directory\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "# create an iterator function to yield text and label\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('in 1974 the teenager martha moxley maggie grace moves to the high class area of belle haven greenwich connecticut on the mischief night eve of halloween she was murdered in the backyard of her house and her murder remained unsolved twenty two years later the writer mark fuhrman christopher meloni who is a former la detective that has fallen in disgrace for perjury in o j simpson trial and moved to idaho decides to investigate the case with his partner stephen weeks andrew mitchell with the purpose of writing a book the locals squirm and do not welcome them but with the support of the retired detective steve carroll robert forster that was in charge of the investigation in the 70 s they discover the criminal and a net of power and money to cover the murder murder in greenwich is a good tv movie with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a kennedy the powerful and rich family used their influence to cover the murder for more than twenty years however a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed the screenplay shows the investigation of mark and the last days of martha in parallel but there is a lack of the emotion in the dramatization my vote is seven title brazil not available',\n",
       " 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use next function to get the next document from the iterator\n",
    "next(stream_docs(path=file))\n",
    "# should return a tuple of (text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes stream_docs function and return a number of documents specified by size\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer\n",
    "- can't use `CountVectorizer` and `TfidfVectorizer` for out-of-core learning\n",
    "    - they require holding the complete vocabulary and documents in memory\n",
    "- `HashingVectorizer` is data-independent and makes use of the hashing trick via 32-bit MurmurShash3 algorithm\n",
    "- difference between `CountVectorizer` and `HashingVectorizer`: https://kavita-ganesan.com/hashingvectorizer-vs-countvectorizer/#.YFF_lbRKhTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# create HashingVectorizer object with 2**21 max slots\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "\n",
    "doc_stream = stream_docs(path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:20\n"
     ]
    }
   ],
   "source": [
    "# let's traing the model in batch; display the status with pyprind library\n",
    "# takes about 20 seconds\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "# use 45 mini batches each with 1000 documents\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "# let's use the last 5000 samples to test our model\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- eventhough the accuracy is slighly lower compared to offline learning with grid search technique, training time is much faster!\n",
    "- we can incrementally train the model with more data\n",
    "    - let's use the 5000 test samples we've not used to train the model yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.884\n"
     ]
    }
   ],
   "source": [
    "# let test the model again out of curiosity\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "# accuracy went up by about 2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing fitted scikit-learn estimators\n",
    "- training a machine learning algorithm can be computationally expensive\n",
    "- don't want to retrain our model every time we close our Python interpreter and want to make a new prediction or reload our web application\n",
    "- one option is to use Python's `pickle` module\n",
    "    - `pickle` can serilaize and deserialize Python object structures to compact bytecode\n",
    "    - save our classifier in its current state and reload it if we want to classify new, unlabeled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "dest = './demos/movieclassifier/pkl_objects'\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "# let's serialize the stop-word set\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)\n",
    "# let's serialize the trained classifier\n",
    "pickle.dump(clf,\n",
    "open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demos/movieclassifier/vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/movieclassifier/vectorizer.py\n",
    "# the above Jupyter notebook magic writes the code in the cell to the provided file; must be the first line!\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's deserialize the pickle objects and test them\n",
    "# change the current working directory to demos/movieclassifier\n",
    "import os\n",
    "os.chdir('demos/movieclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deserialize the classifer\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(label, prob):\n",
    "    if label == 1:\n",
    "        if prob >= 90:\n",
    "            return ':D'\n",
    "        elif prob >= 70:\n",
    "            return ':)'\n",
    "        else:\n",
    "            return ':|'\n",
    "    else:\n",
    "        if prob >= 90:\n",
    "            return ':`('\n",
    "        elif prob >= 70:\n",
    "            return ':('\n",
    "        else:\n",
    "            return ':|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 95.55%\n",
      "Result:  :D\n"
     ]
    }
   ],
   "source": [
    "# let's test the classifier with some reviews\n",
    "import numpy as np\n",
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = [\"I love this movie. It's amazing.\"]\n",
    "X = vect.transform(example)\n",
    "# predict returns the class label with the largest probability\n",
    "lbl = clf.predict(X)\n",
    "# predict_prob method returns the probability estimate for the sample\n",
    "prob = np.max(clf.predict_proba(X))*100\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[lbl[0]], prob))\n",
    "print('Result: ', result(lbl, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 94.56%\n",
      "Result:  :`(\n"
     ]
    }
   ],
   "source": [
    "example = [\"The movie was so boring that I slept through it!\"]\n",
    "X = vect.transform(example)\n",
    "lbl = clf.predict(X)\n",
    "prob = np.max(clf.predict_proba(X))*100\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[lbl[0]], prob))\n",
    "print('Result: ', result(lbl, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 64.22%\n",
      "Result:  :|\n"
     ]
    }
   ],
   "source": [
    "example = [\"The movie was okay but I'd not watch it again!\"]\n",
    "X = vect.transform(example)\n",
    "lbl = clf.predict(X)\n",
    "prob = np.max(clf.predict_proba(X))*100\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[lbl[0]], prob))\n",
    "print('Result: ', result(lbl, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web application with Flask\n",
    "- create a new virtual environment with python 3.9 called `flask` with conda\n",
    "- activate and install `Flask` framework `gunicorn` web server in `flask` virtual environment\n",
    "```bash\n",
    "conda create -n flask python=3.9\n",
    "conda activate flask\n",
    "pip install flask gunicorn\n",
    "```\n",
    "- gunicron is recommended web server for deploy Flask app in Heroku\n",
    "    - see: https://devcenter.heroku.com/articles/python-gunicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World App\n",
    "- follow direction here - https://flask.palletsprojects.com/en/1.1.x/quickstart/#a-minimal-application\n",
    "- flask provides development server\n",
    "\n",
    "```\n",
    "cd <project folder>\n",
    "export FLASK_APP=<flaskapp.py>\n",
    "flask run\n",
    "```\n",
    "\n",
    "- don't need to export `FLASK_APP` env variable if the main module is named `app`\n",
    "- run local web server with gunicorn\n",
    "\n",
    "```bash\n",
    "cd <project folder>\n",
    "gunicorn <flaskapp>:app\n",
    "```\n",
    "- see complete hello world app here: https://github.com/rambasnet/flask-hello\n",
    "\n",
    "### Deploy Flask App\n",
    "- see various options here - https://flask.palletsprojects.com/en/1.1.x/deploying/#deployment\n",
    "- create Heroku account\n",
    "- create an app on heroku\n",
    "- download and install Heroku CLI - https://devcenter.heroku.com/articles/heroku-cli\n",
    "- add heroko to existing git repo or create a new one repo and add heroku\n",
    "- see the deployed app in heroku: https://rb-flask-hello.herokuapp.com/\n",
    "    \n",
    "- create `requirements.txt` file with Python dependencies\n",
    "```bash\n",
    "cd <projectRepo>\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "- create runtime.txt file and add python version that's supported by Heroku (similar to local version)\n",
    "```\n",
    "python-3.9.4\n",
    "```\n",
    "\n",
    "- create `Procfile` and add the following contents:\n",
    "\n",
    "```\n",
    "web: gunicorn hello:app\n",
    "```\n",
    "- `IMPORTANT` - note the required space before `gunicorn`\n",
    "    - app will not launch without it as of April 12, 2021\n",
    "- tell Heroku to run web server with gunicorn hello.py as the main app file\n",
    "\n",
    "- deploy the app using heroku CLI\n",
    "- must add and commit to your repository first before pushing to heroku\n",
    "\n",
    "```bash\n",
    "git add <file...>\n",
    "git commit -m \"...\"\n",
    "git push\n",
    "git push heroku main # push the contents to heroku\n",
    "```\n",
    "- if successfully deployed, visit `<app-name>.herokuapp.com` or run the app from your Heroku dashboard\n",
    "\n",
    "### Demo applications\n",
    "- `demos/flask_app_1` - a simple app with template\n",
    "    - install required dependencies using the provided requirement file\n",
    "\n",
    "```bash\n",
    "cd demos/movieclassifier\n",
    "pip install -r requirements.txt\n",
    "export FLASK_ENV=developement\n",
    "flask run\n",
    "```\n",
    "\n",
    "- `demos/flask_app_2` - a Flask app with form\n",
    "    - install required dependencies using the provided requirement file\n",
    "\n",
    "```bash\n",
    "cd demos/movieclassifier\n",
    "pip install -r requirements.txt\n",
    "export FLASK_ENV=developement\n",
    "flask run\n",
    "```\n",
    "\n",
    "- `demos/movieclassifier` - ML deployed app\n",
    "    - install required dependencies using the provided requirement file\n",
    "\n",
    "```bash\n",
    "cd demos/movieclassifier\n",
    "pip install -r requirements.txt\n",
    "export FLASK_ENV=developement\n",
    "flask run\n",
    "```\n",
    "\n",
    "- `demos/movieclassifier_with_update` - ML deployed app with model update on the fly\n",
    "    - install required dependencies using the provided requirement file\n",
    "    \n",
    "```bash\n",
    "cd demos/movieclassifier\n",
    "pip install -r requirements.txt\n",
    "export FLASK_ENV=developement\n",
    "flask run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
